{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Influence of translation on Poetry Style Assessment\n",
    "\n",
    "\n",
    "This project aims to investigate the structure of poetry in terms of original structure of the poems with respect to existing corpus. We shall concentrate on Chakspeare HAMLET, with a comparison between its original English version and French translation. We are comparing the preservation of inherent stylistic properties through this translation.  \n",
    "\n",
    "## Task 1\n",
    "\n",
    "> 1. Write a script that allows you to retrieve the text corresponding to HAMLET (character) sayings in the manuscript, while discarding stopwords and numbering and any non-related text. Separate the sayings of HAMLET character at each act for both original text and translation.  \n",
    "\n",
    "Analyzation of Hamlet EN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/kali/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('stopwords')  \n",
    "\n",
    "file1 = open(\"HamletEN.txt\", \"r\")\n",
    "\n",
    "\n",
    "playEN = [[],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        []]\n",
    "\n",
    "act = 0\n",
    "\n",
    "#skips the play information\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]) == \"ACT\":\n",
    "                break\n",
    "            else:\n",
    "                pass       \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "#get sentences. Contains stage directions\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]) == \"ACT\":\n",
    "                act += 1\n",
    "            if (line[0]+line[1]+line[2]+line[3]+line[4]+line[5]) == \"HAMLET\":\n",
    "                if line.split(\"HAMLET\", 1)[1] != \"\\n\":\n",
    "                    playEN[act].append(line.split(\"HAMLET\", 1)[1].rstrip())\n",
    "                line = file1.readline()\n",
    "                while line and line != '\\n' :\n",
    "                    playEN[act].append(line.rstrip())\n",
    "                    line = file1.readline()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    else :\n",
    "        file1.close()\n",
    "        break \n",
    "\n",
    "#remove stage direction\n",
    "\n",
    "noDirectPlayEN = [[x for x in playEN[0] if \"[\" not in x],\n",
    "                  [x for x in playEN[1] if \"[\" not in x],\n",
    "                  [x for x in playEN[2] if \"[\" not in x],\n",
    "                  [x for x in playEN[3] if \"[\" not in x],\n",
    "                  [x for x in playEN[4] if \"[\" not in x]]\n",
    "\n",
    "#remove special characters\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "noSpecialCharEN = [[''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayEN[0]],\n",
    "                   [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayEN[1]],\n",
    "                   [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayEN[2]],\n",
    "                   [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayEN[3]],\n",
    "                   [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayEN[4]]]\n",
    "\n",
    "#make everything lower case\n",
    "\n",
    "allLowerEN = [[ x.lower() for x in noSpecialCharEN[0]],\n",
    "              [ x.lower() for x in noSpecialCharEN[1]],\n",
    "              [ x.lower() for x in noSpecialCharEN[2]],\n",
    "              [ x.lower() for x in noSpecialCharEN[3]],\n",
    "              [ x.lower() for x in noSpecialCharEN[4]]]\n",
    "\n",
    "#tokenize\n",
    "\n",
    "tokensEN = [[ nltk.word_tokenize(x) for x in allLowerEN[0]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[1]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[2]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[3]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[4]]]\n",
    "        \n",
    "#remove stopwords\n",
    "\n",
    "stop_wordsEN = set(line.strip() for line in open('stop_words_english.txt'))\n",
    "noStopwEN = [[[t for t in x if t not in stop_wordsEN ]  for x in tokensEN[0]],\n",
    "            [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[1]],\n",
    "            [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[2]],\n",
    "            [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[3]],\n",
    "            [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[4]]]\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmaEN = [[ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[0]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[1]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[2]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[3]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[4]]]\n",
    "\n",
    "\n",
    "\n",
    "#remove act and line seperation\n",
    "wordsEN = []\n",
    "for act in lemmaEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            wordsEN.append(word)\n",
    "\n",
    "#unique words only\n",
    "def unique(list1):\n",
    "  \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "  \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    # print list\n",
    "    return unique_list\n",
    "\n",
    "uniqueEN = unique(wordsEN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzation of Hamlet FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "file1 = open(\"HamletFR.txt\", \"r\")\n",
    "\n",
    "\n",
    "playFR =[[],\n",
    "         [],\n",
    "         [],\n",
    "         [],\n",
    "         []]\n",
    "\n",
    "act = 0\n",
    "\n",
    "#skips the play information\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]+line[3]) == \"ACTE\":\n",
    "                break\n",
    "            else:\n",
    "                pass       \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "#get sentences\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]+line[3]) == \"ACTE\":\n",
    "                act += 1\n",
    "            if (line[0]+line[1]+line[2]+line[3]+line[4]+line[5]) == \"HAMLET\":\n",
    "                if line.split(\"— \", 1)[1] != \"\\n\":\n",
    "                    playFR [act].append(line.split(\"— \", 1)[1].rstrip())\n",
    "                line = file1.readline()\n",
    "                while line and line != '\\n' :\n",
    "                    playFR [act].append(line.rstrip())\n",
    "                    line = file1.readline()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    else :\n",
    "        file1.close()\n",
    "        break \n",
    "\n",
    "\n",
    "#remove special characters\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyzàâèéêëîïôùûüç\\'- ÀÂÈÉÊËÎÏÔABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "noSpecialCharFR  = [[''.join(filter(whitelist.__contains__, x)) for x in playFR[0]],\n",
    "                    [''.join(filter(whitelist.__contains__, x)) for x in playFR[1]],\n",
    "                    [''.join(filter(whitelist.__contains__, x)) for x in playFR[2]],\n",
    "                    [''.join(filter(whitelist.__contains__, x)) for x in playFR[3]],\n",
    "                    [''.join(filter(whitelist.__contains__, x)) for x in playFR[4]]]\n",
    "\n",
    "#make everything lower case\n",
    "\n",
    "allLowerFR  = [[ x.lower() for x in noSpecialCharFR[0]],\n",
    "               [ x.lower() for x in noSpecialCharFR[1]],\n",
    "               [ x.lower() for x in noSpecialCharFR[2]],\n",
    "               [ x.lower() for x in noSpecialCharFR[3]],\n",
    "               [ x.lower() for x in noSpecialCharFR[4]]]\n",
    "\n",
    "#tokenize\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokensFR = [[ [t.text for t in tokenizer(x)] for x in allLowerFR[0]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[1]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[2]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[3]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[4]]]\n",
    "     \n",
    "#remove stopwords\n",
    "\n",
    "stop_wordsFR = set(line.strip() for line in open('stop_words_french.txt'))\n",
    "noStopwFR = [[[t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[0]],\n",
    "            [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[1]],\n",
    "            [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[2]],\n",
    "            [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[3]],\n",
    "            [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[4]]]\n",
    "\n",
    "\n",
    "#lemmatization. Removes act and line seperation\n",
    "\n",
    "lemmaFR = []\n",
    "\n",
    "for act in noStopwFR:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            doc = nlp(word)\n",
    "            for token in doc:\n",
    "                lemmaFR.append(token.lemma_)\n",
    "\n",
    "\n",
    "uniqueFR = unique(lemmaFR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2\n",
    "\n",
    ">2.\tUse appropriate Tokenizer to perform the standard preprocessing pipeline (eliminate stopwords, numbers, uncommon characters,..) and recover the root form of individual words using WordNet lemmatizer. Generate the corresponding vocabulary for both English and French corpus and save it a database. Compare the size of the vocabulary of English Corpus and French corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2083\n",
      "2259\n"
     ]
    }
   ],
   "source": [
    "print(len(uniqueEN))\n",
    "print(len(uniqueFR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3\n",
    "\n",
    "> 3. Use NLTK tokenizer to distinguish various tokens in each text and suggest a script that calculates average length per line in terms of number of characters, and determine the distribution of the lengths after histogram illustration (you may consult NLTK online book for examples). Draw on the same plot the distribution of English and French corpus.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "averagesEN = []\n",
    "length = 0\n",
    "\n",
    "for act in tokensEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            length += len(word)\n",
    "        averagesEN.append(round(length / len(line), 2))\n",
    "        length = 0\n",
    "\n",
    "averagesFR = []\n",
    "length = 0\n",
    "\n",
    "for act in tokensFR:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            length += len(word)\n",
    "        averagesFR.append(round(length / len(line), 2))\n",
    "        length = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "q25, q75 = np.percentile(averagesEN, [25, 75])\n",
    "bin_width = 2 * (q75 - q25) * len(averagesEN) ** (-1/3)\n",
    "bins = round((max(averagesEN)- min(averagesEN)) / bin_width)\n",
    "\n",
    "sns.histplot(averagesEN, bins=bins, ax=ax, kde=True);\n",
    "sns.histplot(averagesFR, bins=bins, ax=ax,  kde=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4\n",
    "\n",
    ">4.\tSuggest a script that draws and estimate Zipf’s law fitting using all data of HAMLET (character) corpus for both English and French (draw on the same plot English and French Zipf’s law fitting (you may consult Project 1 description and links to Zipf’s law fitting and confidence estimation). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "zipWords = []\n",
    "\n",
    "frequency = []\n",
    "\n",
    "#Vaihto tassa kielien valilla\n",
    "for act in noStopwFR:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            zipWords.append(word)\n",
    "\n",
    "frequency = {}\n",
    "\n",
    "for word in zipWords:\n",
    "    count = frequency.get(word , 0)\n",
    "    frequency[ word ] = count + 1\n",
    "\n",
    "rank = 1\n",
    "column_header = ['Rank', 'Frequency', 'Frequency * Rank']\n",
    "df = pd.DataFrame( columns = column_header )\n",
    "collection = sorted(frequency.items(), key=itemgetter(1), reverse = True)\n",
    "\n",
    "for word , freq in collection:\n",
    "    df.loc[word] = [rank, freq, rank*freq]\n",
    "    rank = rank + 1\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "\n",
    "sns.regplot(x=\"Rank\", y=\"Frequency\",  data=np.log10(df[:20].astype('float64')), ax=ax)\n",
    "\n",
    "\n",
    "formatter = lambda x, pos: f'{10 ** x:g}'\n",
    "ax.get_xaxis().set_major_formatter(formatter)\n",
    "ax.get_yaxis().set_major_formatter(formatter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 5\n",
    "\n",
    ">5.\tNow we want to assess the coherence each line of English corpus with its counterpart in French corpus. Consider FastText embedding, see Word vectors for 157 languages · fastText,  which is available for several languages.  Write a script that calculates the embedding of the whole line as the average of the FastText embedding of individual words constituting the line. For French corpus, you should use the French embedding available from the above link. Now given the word embedding of a given line in English and its corresponding in French corpus, the consistency score is calculated as the cosine similarity between the two embedding vector.  Then, display in a graph the variation of the consistency score for the whole corpus. Report in a table the mean, standard deviation, kurtosis and skewness values of the consistency scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
