{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Influence of translation on Poetry Style Assessment\n",
    "\n",
    "This notebook covers the comparation of the inherent stylistic properties of Shakespeare's Hamlet. \n",
    "This work is conducted as a part of course *Natural Language Processing and Text Mining* (521158S), taught in the *University of Oulu* in autumn 2022\n",
    "\n",
    "## Authors\n",
    "\n",
    "   - Olli\n",
    "       - Insert\n",
    "   - Saku\n",
    "       - Studentnummer\n",
    "   - Joose\n",
    "       - i här\n",
    "   \n",
    "\n",
    "## Introduction \n",
    "\n",
    "This project aims to investigate the structure of poetry in terms of original structure of the poems with respect to existing corpus. We shall concentrate on Chakspeare HAMLET, with a comparison between its original English version and French translation. We are comparing the preservation of inherent stylistic properties through this translation.  \n",
    "\n",
    "## Task 1\n",
    "\n",
    "> 1. Write a script that allows you to retrieve the text corresponding to HAMLET (character) sayings in the manuscript, while discarding stopwords and numbering and any non-related text. Separate the sayings of HAMLET character at each act for both original text and translation.  \n",
    "\n",
    "Analyzation of Hamlet EN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('stopwords')  \n",
    "\n",
    "file1 = open(\"HamletEN.txt\", \"r\")\n",
    "\n",
    "\n",
    "\n",
    "seperate = []\n",
    "\n",
    "playEN = [[],\n",
    "          [],\n",
    "          [],\n",
    "          [],\n",
    "          []]\n",
    "\n",
    "act = 0\n",
    "\n",
    "\n",
    "#skips the play information\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]) == \"ACT\":\n",
    "                break\n",
    "            else:\n",
    "                pass       \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "#get sentences. Contains stage directions\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]) == \"ACT\":\n",
    "                act += 1\n",
    "            if (line[0]+line[1]+line[2]+line[3]+line[4]+line[5]) == \"HAMLET\":\n",
    "                if line.split(\"HAMLET\", 1)[1] != \"\\n\":\n",
    "                    seperate.append(line.split(\"HAMLET\", 1)[1].rstrip())\n",
    "                line = file1.readline()\n",
    "                while line != '\\n' :\n",
    "                    seperate.append(line.rstrip())\n",
    "                    line = file1.readline()\n",
    "                thisLine = ' '.join(seperate)\n",
    "                playEN[act].append(thisLine)\n",
    "                seperate = []              \n",
    "        except IndexError:\n",
    "            pass\n",
    "    else :\n",
    "        file1.close()\n",
    "        break \n",
    "        \n",
    "\n",
    "#remove special characters except brackets which contain stage direction\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyz\\[ \\]\\-- ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "noSpecialCharPrepEN = [[\"\".join(filter(whitelist.__contains__, x)) for x in playEN[0]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x)) for x in playEN[1]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x)) for x in playEN[2]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x)) for x in playEN[3]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x)) for x in playEN[4]]]\n",
    "\n",
    "#Replace \"--\" with a whitespace\n",
    "noSpecialCharEN = [[re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[0]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[1]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[2]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[3]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[4]]]               \n",
    "\n",
    "\n",
    "\n",
    "#remove stage direction\n",
    "noDirectPlayPrepEN = [[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[0]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[1]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[2]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[3]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[4]]]\n",
    "\n",
    "# Removes extra whitespaces\n",
    "noDirectPlayEN = [[re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[0]],\n",
    "                  [re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[1]],\n",
    "                  [re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[2]],\n",
    "                  [re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[3]],\n",
    "                  [re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[4]]]\n",
    "\n",
    "\n",
    "#make everything lower case\n",
    "allLowerEN = [[ x.lower() for x in noDirectPlayEN[0]],\n",
    "              [ x.lower() for x in noDirectPlayEN[1]],\n",
    "              [ x.lower() for x in noDirectPlayEN[2]],\n",
    "              [ x.lower() for x in noDirectPlayEN[3]],\n",
    "              [ x.lower() for x in noDirectPlayEN[4]]]\n",
    "\n",
    "#tokenize\n",
    "tokensEN = [[ nltk.word_tokenize(x) for x in allLowerEN[0]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[1]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[2]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[3]],\n",
    "            [ nltk.word_tokenize(x) for x in allLowerEN[4]]]\n",
    "        \n",
    "#remove stopwords\n",
    "stop_wordsEN = set(line.strip() for line in open('stop_words_english.txt', encoding=\"utf8\"))\n",
    "noStopwprepEN = [[[t for t in x if t not in stop_wordsEN ]  for x in tokensEN[0]],\n",
    "                 [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[1]],\n",
    "                 [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[2]],\n",
    "                 [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[3]],\n",
    "                 [ [t for t in x if t not in stop_wordsEN ]  for x in tokensEN[4]]]\n",
    "\n",
    "#remove empty lines\n",
    "noStopwEN = [[ x for x in noStopwprepEN[0] if x != []],\n",
    "             [ x for x in noStopwprepEN[1] if x != []],\n",
    "             [ x for x in noStopwprepEN[2] if x != []],\n",
    "             [ x for x in noStopwprepEN[3] if x != []],\n",
    "             [ x for x in noStopwprepEN[4] if x != []]]\n",
    "\n",
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmaEN = [[ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[0]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[1]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[2]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[3]],\n",
    "           [ [lemmatizer.lemmatize(t) for t in x ]  for x in noStopwEN[4]]]\n",
    "\n",
    "\n",
    "\n",
    "#remove act and line seperation\n",
    "wordsEN = []\n",
    "for act in lemmaEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            wordsEN.append(word)\n",
    "\n",
    "#unique words only\n",
    "def unique(list1):\n",
    "  \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "  \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    # print list\n",
    "    return unique_list\n",
    "\n",
    "uniqueEN = unique(wordsEN)\n",
    "\n",
    "\n",
    "print(noDirectPlayEN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzation of Hamlet FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import fr_core_news_md #HOX, python -m spacy download fr_core_news_md\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "file1 = open(\"HamletFR.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "playFR =[[],\n",
    "         [],\n",
    "         [],\n",
    "         [],\n",
    "         []]\n",
    "\n",
    "act = 0\n",
    "\n",
    "#skips the play information\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]+line[3]) == \"ACTE\":\n",
    "                break\n",
    "            else:\n",
    "                pass       \n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "#get sentences\n",
    "while True:\n",
    "    line = file1.readline() \n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]+line[3]) == \"ACTE\":\n",
    "                act += 1\n",
    "            if (line[0]+line[1]+line[2]+line[3]+line[4]+line[5]) == \"HAMLET\":\n",
    "                if line.split(\"— \", 1)[1] != \"\\n\":\n",
    "                    playFR [act].append(line.split(\"— \", 1)[1].rstrip())\n",
    "                line = file1.readline()\n",
    "                while line and line != '\\n' :\n",
    "                    playFR [act].append(line.rstrip())\n",
    "                    line = file1.readline()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    else :\n",
    "        file1.close()\n",
    "        break \n",
    "\n",
    "#remove stagedirections\n",
    "noDirectPlayFR = [[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[0]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[1]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[2]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[3]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[4]]]\n",
    "\n",
    "\n",
    "#remove special characters\n",
    "whitelist = set('abcdefghijklmnopqrstuvwxyzàâèéêëîïôùûüç\\'- ÀÂÈÉÊËÎÏÔABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "noSpecialCharPrepFR  = [[''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR [0]],\n",
    "                        [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR [1]],\n",
    "                        [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR [2]],\n",
    "                        [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR [3]],\n",
    "                        [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR [4]]]\n",
    "\n",
    "noSpecialCharFR  = [[re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[0]],\n",
    "                    [re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[1]],\n",
    "                    [re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[2]],\n",
    "                    [re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[3]],\n",
    "                    [re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[4]]]\n",
    "\n",
    "#make everything lower case\n",
    "\n",
    "allLowerFR  = [[ x.lower() for x in noSpecialCharFR[0]],\n",
    "               [ x.lower() for x in noSpecialCharFR[1]],\n",
    "               [ x.lower() for x in noSpecialCharFR[2]],\n",
    "               [ x.lower() for x in noSpecialCharFR[3]],\n",
    "               [ x.lower() for x in noSpecialCharFR[4]]]\n",
    "\n",
    "#tokenize 55play\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokensFR = [[ [t.text for t in tokenizer(x)] for x in allLowerFR[0]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[1]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[2]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[3]],\n",
    "            [ [t.text for t in tokenizer(x)]  for x in allLowerFR[4]]]\n",
    "     \n",
    "#remove stopwords\n",
    "\n",
    "stop_wordsFR = set(line.strip() for line in open('stop_words_french.txt', encoding=\"utf8\"))\n",
    "noStopwprepFR = [[[t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[0]],\n",
    "                 [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[1]],\n",
    "                 [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[2]],\n",
    "                 [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[3]],\n",
    "                 [ [t for t in x if t not in stop_wordsFR and t != ' ']  for x in tokensFR[4]]]\n",
    "\n",
    "noStopwFR = [[ x for x in noStopwprepFR[0] if x != []],\n",
    "             [ x for x in noStopwprepFR[1] if x != []],\n",
    "             [ x for x in noStopwprepFR[2] if x != []],\n",
    "             [ x for x in noStopwprepFR[3] if x != []],\n",
    "             [ x for x in noStopwprepFR[4] if x != []]]\n",
    "\n",
    "\n",
    "#lemmatization. Removes act and line seperation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmaFR = []\n",
    "\n",
    "for act in noStopwFR:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            doc = nlp(word)\n",
    "            for token in doc:\n",
    "                lemmaFR.append(token.lemma_)\n",
    "\n",
    "                \n",
    "\n",
    "uniqueFR = unique(lemmaFR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2\n",
    "\n",
    ">2.\tUse appropriate Tokenizer to perform the standard preprocessing pipeline (eliminate stopwords, numbers, uncommon characters,..) and recover the root form of individual words using WordNet lemmatizer. Generate the corresponding vocabulary for both English and French corpus and save it a database. Compare the size of the vocabulary of English Corpus and French corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(uniqueEN))\n",
    "print(len(uniqueFR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3\n",
    "\n",
    "> 3. Use NLTK tokenizer to distinguish various tokens in each text and suggest a script that calculates average length per line in terms of number of characters, and determine the distribution of the lengths after histogram illustration (you may consult NLTK online book for examples). Draw on the same plot the distribution of English and French corpus.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "averagesEN = []\n",
    "length = 0\n",
    "\n",
    "for act in tokensEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            length += len(word)\n",
    "        averagesEN.append(round(length / len(line), 2))\n",
    "        length = 0\n",
    "\n",
    "averagesFR = []\n",
    "length = 0\n",
    "\n",
    "for act in tokensFR:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            length += len(word)\n",
    "        averagesFR.append(round(length / len(line), 2))\n",
    "        length = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "q25, q75 = np.percentile(averagesEN, [25, 75])\n",
    "bin_width = 2 * (q75 - q25) * len(averagesEN) ** (-1/3)\n",
    "bins = round((max(averagesEN)- min(averagesEN)) / bin_width)\n",
    "\n",
    "sns.histplot(averagesEN, bins=bins, ax=ax, kde=False);\n",
    "sns.histplot(averagesFR, bins=bins, ax=ax,  kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 4\n",
    "\n",
    ">4.\tSuggest a script that draws and estimate Zipf’s law fitting using all data of HAMLET (character) corpus for both English and French (draw on the same plot English and French Zipf’s law fitting (you may consult Project 1 description and links to Zipf’s law fitting and confidence estimation). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import operator\n",
    "\n",
    "zipWords = []\n",
    "\n",
    "frequency = []\n",
    "https://github.com/stopwords-iso/stopwords-fr/blob/master/stopwords-fr.txt\n",
    "#Vaihto tassa kielien valilla\n",
    "for act in noStopwEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            zipWords.append(word)\n",
    "\n",
    "frequency = {}\n",
    "\n",
    "for word in zipWords:\n",
    "    count = frequency.get(word , 0)\n",
    "    frequency[ word ] = count + 1\n",
    "\n",
    "rank = 1\n",
    "column_header = ['Rank', 'Frequency', 'Frequency * Rank']\n",
    "df = pd.DataFrame( columns = column_header )\n",
    "collection = sorted(frequency.items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "for word , freq in collection:\n",
    "    df.loc[word] = [rank, freq, rank*freq]\n",
    "    rank = rank + 1\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "\n",
    "g = sns.regplot(x=\"Rank\", y=\"Frequency\",  data=np.log10(df[:20].astype('float64')), ax=ax)\n",
    "\n",
    "\n",
    "formatter = lambda x, pos: f'{10 ** x:g}'\n",
    "ax.get_xaxis().set_major_formatter(formatter)\n",
    "ax.get_yaxis().set_major_formatter(formatter)\n",
    "\n",
    "kuva = g.get_figure()\n",
    "fig.savefig(\"out.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 5\n",
    "\n",
    ">5.\tNow we want to assess the coherence each line of English corpus with its counterpart in French corpus. Consider FastText embedding, see Word vectors for 157 languages · fastText,  which is available for several languages.  Write a script that calculates the embedding of the whole line as the average of the FastText embedding of individual words constituting the line. For French corpus, you should use the French embedding available from the above link. Now given the word embedding of a given line in English and its corresponding in French corpus, the consistency score is calculated as the cosine similarity between the two embedding vector.  Then, display in a graph the variation of the consistency score for the whole corpus. Report in a table the mean, standard deviation, kurtosis and skewness values of the consistency scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.\t\n",
    "\n",
    ">We want to test the coherence in terms of sentiment analysis score. For this purpose, use Textblob sentiment analyzer as it supports several languages (see example at French Sentiment Analysis Using TextBlob | Kaggle, which outputs for a given text input, positive, negative or neutral). For each line of English and French corpus, output the overall sentiment score. Then assign a Boolean output of 1 if the sentiment of both lines match and zero, otherwise. Draw the (0-1) histogram showing the proportion of matches and mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "senti_listEN, senti_listFR = [], []\n",
    "lines_sentiEN, lines_SentiFR = [], []\n",
    "\n",
    "#TODO: tokensEN sisältää mitä?\n",
    "for act in tokensEN:\n",
    "    for j, line in enumerate(act):\n",
    "        lineSentiment = 0\n",
    "        print(\"line {} which contains: {}\".format(j, line))\n",
    "        for i in line:\n",
    "            vs = tb(i).sentiment[1]\n",
    "            if (vs > 0):\n",
    "                print(\"word \"+i+\" is positive\")\n",
    "                senti_listEN.append('Positive')\n",
    "                lineSentiment =+ vs\n",
    "            elif (vs < 0):\n",
    "                print(\"word \"+i+\" is negative\")\n",
    "                senti_listEN.append('Negative')\n",
    "                lineSentiment =- vs\n",
    "            else:\n",
    "                #print(\"word \"+i+\" is neutral\")\n",
    "                senti_listEN.append('Neutral')   \n",
    "                lineSentiment =- vs\n",
    "        print(\"This particular lines sentiment is: {}\".format(lineSentiment))        \n",
    "        lines_sentiEN.append(lineSentiment)\n",
    "            \n",
    "\n",
    "for act in tokensFR:\n",
    "    for j, line in enumerate(act):\n",
    "        lineSentiment = 0\n",
    "        print(\"line {} which contains: {}\".format(j, line))\n",
    "        for i in line:\n",
    "            vs = tb(i).sentiment[1]\n",
    "            if (vs > 0):\n",
    "                print(\"word \"+i+\" is positive\")\n",
    "                senti_listFR.append('Positive')\n",
    "                lineSentiment =+ vs\n",
    "            elif (vs < 0):\n",
    "                print(\"word \"+i+\" is negative\")\n",
    "                senti_listFR.append('Negative')\n",
    "                lineSentiment =- vs\n",
    "            else:\n",
    "                #print(\"word \"+i+\" is neutral\")\n",
    "                senti_listFR.append('Neutral')   \n",
    "                lineSentiment =- vs\n",
    "        print(\"This particular lines sentiment is: {}\".format(lineSentiment))        \n",
    "        lines_sentiFR.append(lineSentiment)            \n",
    "                \n",
    "\n",
    "dataEN=senti_listEN\n",
    "dataSentiEN = lines_sentiEN\n",
    "\n",
    "dataFR = senti_listFR     \n",
    "dataSentiFR = lines_sentiFR        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataSentiEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = dataEN, dataFR\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=dataEN, name=\"EN\"))\n",
    "fig.add_trace(go.Histogram(x=dataFR, name=\"FR\"))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout()\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "fig3 = px.line(dataSentiEN, title='Line sentiment in English')\n",
    "fig4 = px.line(dataSentiFR, title='Line sentiment in French')\n",
    "\n",
    "\n",
    "fig3.show()\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
