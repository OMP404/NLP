{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Influence of translation on Poetry Style Assessment\n",
    "\n",
    "This notebook covers the comparation of the inherent stylistic properties of Shakespeare's Hamlet.\n",
    "This work is conducted as a part of course _Natural Language Processing and Text Mining_ (521158S), taught in the _University of Oulu_ in autumn 2022\n",
    "\n",
    "## Authors\n",
    "\n",
    "-   Olli\n",
    "    -   Insert\n",
    "-   Saku\n",
    "    -   Studentnummer\n",
    "-   Joose\n",
    "    -   i här\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project aims to investigate the structure of poetry in terms of original structure of the poems with respect to existing corpus. We shall concentrate on Chakspeare HAMLET, with a comparison between its original English version and French translation. We are comparing the preservation of inherent stylistic properties through this translation.\n",
    "\n",
    "## Task 1\n",
    "\n",
    "> 1. Write a script that allows you to retrieve the text corresponding to HAMLET (character) sayings in the manuscript, while discarding stopwords and numbering and any non-related text. Separate the sayings of HAMLET character at each act for both original text and translation.\n",
    "\n",
    "Analyzation of Hamlet EN:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "file1 = open(\"HamletEN.txt\", \"r\")\n",
    "\n",
    "\n",
    "seperate = []\n",
    "\n",
    "playEN = [[],\n",
    "          [],\n",
    "          [],\n",
    "          [],\n",
    "          []]\n",
    "\n",
    "act = 0\n",
    "\n",
    "\n",
    "# skips the play information\n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]) == \"ACT\":\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "# get sentences. Contains stage directions\n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]) == \"ACT\":\n",
    "                act += 1\n",
    "            if (line[0]+line[1]+line[2]+line[3]+line[4]+line[5]) == \"HAMLET\":\n",
    "                if line.split(\"HAMLET\", 1)[1] != \"\\n\":\n",
    "                    seperate.append(line.split(\"HAMLET\", 1)[1].rstrip())\n",
    "                line = file1.readline()\n",
    "                while line != '\\n':\n",
    "                    seperate.append(line.rstrip())\n",
    "                    line = file1.readline()\n",
    "                thisLine = ' '.join(seperate)\n",
    "                playEN[act].append(thisLine)\n",
    "                seperate = []\n",
    "        except IndexError:\n",
    "            pass\n",
    "    else:\n",
    "        file1.close()\n",
    "        break\n",
    "\n",
    "\n",
    "# remove special characters except brackets which contain stage direction\n",
    "whitelist = set(\n",
    "    'abcdefghijklmnopqrstuvwxyz\\[ \\]\\-- ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "noSpecialCharPrepEN = [[\"\".join(filter(whitelist.__contains__, x)) for x in playEN[0]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x))\n",
    "                        for x in playEN[1]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x))\n",
    "                        for x in playEN[2]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x))\n",
    "                        for x in playEN[3]],\n",
    "                       [\"\".join(filter(whitelist.__contains__, x)) for x in playEN[4]]]\n",
    "\n",
    "# Replace \"--\" with a whitespace\n",
    "noSpecialCharEN = [[re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[0]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip()\n",
    "                    for x in noSpecialCharPrepEN[1]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip()\n",
    "                    for x in noSpecialCharPrepEN[2]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip()\n",
    "                    for x in noSpecialCharPrepEN[3]],\n",
    "                   [re.sub(\"[--]\", \" \", x).strip() for x in noSpecialCharPrepEN[4]]]\n",
    "\n",
    "\n",
    "# remove stage direction\n",
    "noDirectPlayPrepEN = [[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[0]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\n",
    "                       for x in noSpecialCharEN[1]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\n",
    "                       for x in noSpecialCharEN[2]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\n",
    "                       for x in noSpecialCharEN[3]],\n",
    "                      [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in noSpecialCharEN[4]]]\n",
    "\n",
    "# Removes extra whitespaces\n",
    "noDirectPlayEN = [[re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[0]],\n",
    "                  [re.sub(' +', ' ', x).strip()\n",
    "                   for x in noDirectPlayPrepEN[1]],\n",
    "                  [re.sub(' +', ' ', x).strip()\n",
    "                   for x in noDirectPlayPrepEN[2]],\n",
    "                  [re.sub(' +', ' ', x).strip()\n",
    "                   for x in noDirectPlayPrepEN[3]],\n",
    "                  [re.sub(' +', ' ', x).strip() for x in noDirectPlayPrepEN[4]]]\n",
    "\n",
    "\n",
    "# make everything lower case\n",
    "allLowerEN = [[x.lower() for x in noDirectPlayEN[0]],\n",
    "              [x.lower() for x in noDirectPlayEN[1]],\n",
    "              [x.lower() for x in noDirectPlayEN[2]],\n",
    "              [x.lower() for x in noDirectPlayEN[3]],\n",
    "              [x.lower() for x in noDirectPlayEN[4]]]\n",
    "\n",
    "# tokenize\n",
    "tokensEN = [[nltk.word_tokenize(x) for x in allLowerEN[0]],\n",
    "            [nltk.word_tokenize(x) for x in allLowerEN[1]],\n",
    "            [nltk.word_tokenize(x) for x in allLowerEN[2]],\n",
    "            [nltk.word_tokenize(x) for x in allLowerEN[3]],\n",
    "            [nltk.word_tokenize(x) for x in allLowerEN[4]]]\n",
    "\n",
    "# remove stopwords\n",
    "stop_wordsEN = set(line.strip()\n",
    "                   for line in open('stop_words_english.txt', encoding=\"utf8\"))\n",
    "noStopwEN = [[[t for t in x if t not in stop_wordsEN] for x in tokensEN[0]],\n",
    "             [[t for t in x if t not in stop_wordsEN] for x in tokensEN[1]],\n",
    "             [[t for t in x if t not in stop_wordsEN] for x in tokensEN[2]],\n",
    "             [[t for t in x if t not in stop_wordsEN] for x in tokensEN[3]],\n",
    "             [[t for t in x if t not in stop_wordsEN] for x in tokensEN[4]]]\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmaEN = [[[lemmatizer.lemmatize(t) for t in x] for x in noStopwEN[0]],\n",
    "           [[lemmatizer.lemmatize(t) for t in x] for x in noStopwEN[1]],\n",
    "           [[lemmatizer.lemmatize(t) for t in x] for x in noStopwEN[2]],\n",
    "           [[lemmatizer.lemmatize(t) for t in x] for x in noStopwEN[3]],\n",
    "           [[lemmatizer.lemmatize(t) for t in x] for x in noStopwEN[4]]]\n",
    "\n",
    "try:\n",
    "    with open('lemmaEN.pickle', 'wb') as f:\n",
    "        pickle.dump(lemmaEN, f)\n",
    "except:\n",
    "    print(\"Error writing to lemmaEN.pickle\")\n",
    "\n",
    "# remove act and line seperation\n",
    "wordsEN = []\n",
    "for act in lemmaEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            wordsEN.append(word)\n",
    "\n",
    "# unique words only\n",
    "def unique(list1):\n",
    "\n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "\n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    # print list\n",
    "    return unique_list\n",
    "\n",
    "\n",
    "uniqueEN = unique(wordsEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzation of Hamlet FR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import fr_core_news_md  # HOX, python -m spacy download fr_core_news_md\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "file1 = open(\"HamletFR.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "playFR = [[],\n",
    "          [],\n",
    "          [],\n",
    "          [],\n",
    "          []]\n",
    "\n",
    "act = 0\n",
    "\n",
    "# skips the play information\n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]+line[3]) == \"ACTE\":\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "# get sentences\n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if line:\n",
    "        try:\n",
    "            if (line[0]+line[1]+line[2]+line[3]) == \"ACTE\":\n",
    "                act += 1\n",
    "            if (line[0]+line[1]+line[2]+line[3]+line[4]+line[5]) == \"HAMLET\":\n",
    "                if line.split(\"— \", 1)[1] != \"\\n\":\n",
    "                    playFR[act].append(line.split(\"— \", 1)[1].rstrip())\n",
    "                line = file1.readline()\n",
    "                while line and line != '\\n':\n",
    "                    playFR[act].append(line.rstrip())\n",
    "                    line = file1.readline()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    else:\n",
    "        file1.close()\n",
    "        break\n",
    "\n",
    "# remove stagedirections\n",
    "noDirectPlayFR = [[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[0]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\n",
    "                   for x in playFR[1]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\n",
    "                   for x in playFR[2]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip()\n",
    "                   for x in playFR[3]],\n",
    "                  [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip() for x in playFR[4]]]\n",
    "\n",
    "\n",
    "# remove special characters\n",
    "whitelist = set(\n",
    "    'abcdefghijklmnopqrstuvwxyzàâèéêëîïôùûüç\\'- ÀÂÈÉÊËÎÏÔABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "\n",
    "noSpecialCharPrepFR = [[''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR[0]],\n",
    "                       [''.join(filter(whitelist.__contains__, x))\n",
    "                        for x in noDirectPlayFR[1]],\n",
    "                       [''.join(filter(whitelist.__contains__, x))\n",
    "                        for x in noDirectPlayFR[2]],\n",
    "                       [''.join(filter(whitelist.__contains__, x))\n",
    "                        for x in noDirectPlayFR[3]],\n",
    "                       [''.join(filter(whitelist.__contains__, x)) for x in noDirectPlayFR[4]]]\n",
    "\n",
    "noSpecialCharFR = [[re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[0]],\n",
    "                   [re.sub(' +', ' ', x).strip()\n",
    "                    for x in noSpecialCharPrepFR[1]],\n",
    "                   [re.sub(' +', ' ', x).strip()\n",
    "                    for x in noSpecialCharPrepFR[2]],\n",
    "                   [re.sub(' +', ' ', x).strip()\n",
    "                    for x in noSpecialCharPrepFR[3]],\n",
    "                   [re.sub(' +', ' ', x).strip() for x in noSpecialCharPrepFR[4]]]\n",
    "\n",
    "# make everything lower case\n",
    "\n",
    "allLowerFR = [[x.lower() for x in noSpecialCharFR[0]],\n",
    "              [x.lower() for x in noSpecialCharFR[1]],\n",
    "              [x.lower() for x in noSpecialCharFR[2]],\n",
    "              [x.lower() for x in noSpecialCharFR[3]],\n",
    "              [x.lower() for x in noSpecialCharFR[4]]]\n",
    "\n",
    "# tokenize 55play\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "tokensFR = [[[t.text for t in tokenizer(x)] for x in allLowerFR[0]],\n",
    "            [[t.text for t in tokenizer(x)] for x in allLowerFR[1]],\n",
    "            [[t.text for t in tokenizer(x)] for x in allLowerFR[2]],\n",
    "            [[t.text for t in tokenizer(x)] for x in allLowerFR[3]],\n",
    "            [[t.text for t in tokenizer(x)] for x in allLowerFR[4]]]\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "stop_wordsFR = set(line.strip()\n",
    "                   for line in open('stop_words_french.txt', encoding=\"utf8\"))\n",
    "noStopwFR = [[[t for t in x if t not in stop_wordsFR and t != ' '] for x in tokensFR[0]],\n",
    "             [[t for t in x if t not in stop_wordsFR and t != ' ']\n",
    "                 for x in tokensFR[1]],\n",
    "             [[t for t in x if t not in stop_wordsFR and t != ' ']\n",
    "                 for x in tokensFR[2]],\n",
    "             [[t for t in x if t not in stop_wordsFR and t != ' ']\n",
    "                 for x in tokensFR[3]],\n",
    "             [[t for t in x if t not in stop_wordsFR and t != ' '] for x in tokensFR[4]]]\n",
    "\n",
    "\n",
    "# lemmatization. Removes act and line seperation\n",
    "\n",
    "lemmaFR = []\n",
    "wordsFR = []\n",
    "\n",
    "for act in noStopwFR:\n",
    "    act_list = []\n",
    "    for line in act:\n",
    "        line_list = []\n",
    "        for word in line:\n",
    "            doc = nlp(word)\n",
    "            for token in doc:\n",
    "                line_list.append(token.lemma_)\n",
    "                wordsFR.append(token.lemma_)\n",
    "        act_list.append(line_list)\n",
    "    lemmaFR.append(act_list)\n",
    "\n",
    "try:\n",
    "    with open('lemmaFR.pickle', 'wb') as f:\n",
    "        pickle.dump(lemmaFR, f)\n",
    "except:\n",
    "    print(\"Error writing to lemmaFR.pickle\")\n",
    "\n",
    "\n",
    "uniqueFR = unique(lemmaFR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "> 2.  Use appropriate Tokenizer to perform the standard preprocessing pipeline (eliminate stopwords, numbers, uncommon characters,..) and recover the root form of individual words using WordNet lemmatizer. Generate the corresponding vocabulary for both English and French corpus and save it a database. Compare the size of the vocabulary of English Corpus and French corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(uniqueEN))\n",
    "print(len(uniqueFR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "> 3. Use NLTK tokenizer to distinguish various tokens in each text and suggest a script that calculates average length per line in terms of number of characters, and determine the distribution of the lengths after histogram illustration (you may consult NLTK online book for examples). Draw on the same plot the distribution of English and French corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "averagesEN = []\n",
    "length = 0\n",
    "\n",
    "for act in tokensEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            length += len(word)\n",
    "        averagesEN.append(round(length / len(line), 2))\n",
    "        length = 0\n",
    "\n",
    "averagesFR = []\n",
    "length = 0\n",
    "\n",
    "for act in tokensFR:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            length += len(word)\n",
    "        averagesFR.append(round(length / len(line), 2))\n",
    "        length = 0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "q25, q75 = np.percentile(averagesEN, [25, 75])\n",
    "bin_width = 2 * (q75 - q25) * len(averagesEN) ** (-1/3)\n",
    "bins = round((max(averagesEN) - min(averagesEN)) / bin_width)\n",
    "\n",
    "sns.histplot(averagesEN, bins=bins, ax=ax, kde=False)\n",
    "sns.histplot(averagesFR, bins=bins, ax=ax,  kde=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "> 4.  Suggest a script that draws and estimate Zipf’s law fitting using all data of HAMLET (character) corpus for both English and French (draw on the same plot English and French Zipf’s law fitting (you may consult Project 1 description and links to Zipf’s law fitting and confidence estimation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import operator\n",
    "\n",
    "zipWords = []\n",
    "\n",
    "frequency = []\n",
    "# source: https: // github.com/stopwords-iso/stopwords-fr/blob/master/stopwords-fr.txt\n",
    "# Vaihto tassa kielien valilla\n",
    "for act in noStopwEN:\n",
    "    for line in act:\n",
    "        for word in line:\n",
    "            zipWords.append(word)\n",
    "\n",
    "frequency = {}\n",
    "\n",
    "for word in zipWords:\n",
    "    count = frequency.get(word, 0)\n",
    "    frequency[word] = count + 1\n",
    "\n",
    "rank = 1\n",
    "column_header = ['Rank', 'Frequency', 'Frequency * Rank']\n",
    "df = pd.DataFrame(columns=column_header)\n",
    "collection = sorted(frequency.items(),\n",
    "                    key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for word, freq in collection:\n",
    "    df.loc[word] = [rank, freq, rank*freq]\n",
    "    rank = rank + 1\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "\n",
    "g = sns.regplot(x=\"Rank\", y=\"Frequency\",  data=np.log10(\n",
    "    df[:20].astype('float64')), ax=ax)\n",
    "\n",
    "\n",
    "def formatter(x, pos): return f'{10 ** x:g}'\n",
    "\n",
    "\n",
    "ax.get_xaxis().set_major_formatter(formatter)\n",
    "ax.get_yaxis().set_major_formatter(formatter)\n",
    "\n",
    "kuva = g.get_figure()\n",
    "fig.savefig(\"out.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "> 5.  Now we want to assess the coherence each line of English corpus with its counterpart in French corpus. Consider FastText embedding, see Word vectors for 157 languages · fastText, which is available for several languages. Write a script that calculates the embedding of the whole line as the average of the FastText embedding of individual words constituting the line. For French corpus, you should use the French embedding available from the above link. Now given the word embedding of a given line in English and its corresponding in French corpus, the consistency score is calculated as the cosine similarity between the two embedding vector. Then, display in a graph the variation of the consistency score for the whole corpus. Report in a table the mean, standard deviation, kurtosis and skewness values of the consistency scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate script (embedding.py) that only works on linux, recommended atleast 8GB of RAM\n",
    "# gives out similarity scores in similarities.pickle\n",
    "\n",
    "from statistics import mean, stdev\n",
    "import scipy\n",
    "\n",
    "try:\n",
    "    # load cosine similarities of whole lines\n",
    "    with open('similarities.pickle', 'rb') as f:\n",
    "        similarities_withnan = pickle.load(f)\n",
    "except:\n",
    "    print('No pickle file(s) found')\n",
    "\n",
    "# similarities without nan values\n",
    "similarities = [x for x in similarities_withnan if str(x) != 'nan']\n",
    "\n",
    "\n",
    "# calculate table values\n",
    "mean = mean(similarities)\n",
    "std_dev = stdev(similarities)\n",
    "skew = scipy.stats.skew(similarities, axis=0, bias=True)\n",
    "kurtosis = scipy.stats.kurtosis(similarities, axis=0, fisher=True, bias=True)\n",
    "# plot a table\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(similarities)\n",
    "axs[0].set_title('Consistency score variance')\n",
    "columns = (\"Mean\", \"Std Deviation\", \"Kurtosis\", 'Skewness')\n",
    "axs[1].table(cellText=[[mean, std_dev, kurtosis, skew]],\n",
    "             colLabels=columns, loc='center')\n",
    "axs[1].set_title('Consistency score statistics')\n",
    "axs[1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.\n",
    "\n",
    "> We want to test the coherence in terms of sentiment analysis score. For this purpose, use Textblob sentiment analyzer as it supports several languages (see example at French Sentiment Analysis Using TextBlob | Kaggle, which outputs for a given text input, positive, negative or neutral). For each line of English and French corpus, output the overall sentiment score. Then assign a Boolean output of 1 if the sentiment of both lines match and zero, otherwise. Draw the (0-1) histogram showing the proportion of matches and mismatches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "\n",
    "senti_listEN, senti_listFR = [], []\n",
    "lines_sentiEN, lines_sentiFR = [], []\n",
    "\n",
    "# TODO: tokensEN sisältää mitä?\n",
    "for act in tokensEN:\n",
    "    for j, line in enumerate(act):\n",
    "        lineSentiment = 0\n",
    "        print(\"line {} which contains: {}\".format(j, line))\n",
    "        for i in line:\n",
    "            vs = tb(i).sentiment[1]\n",
    "            if (vs > 0):\n",
    "                print(\"word \"+i+\" is positive\")\n",
    "                senti_listEN.append('Positive')\n",
    "                lineSentiment = + vs\n",
    "            elif (vs < 0):\n",
    "                print(\"word \"+i+\" is negative\")\n",
    "                senti_listEN.append('Negative')\n",
    "                lineSentiment = - vs\n",
    "            else:\n",
    "                #print(\"word \"+i+\" is neutral\")\n",
    "                senti_listEN.append('Neutral')\n",
    "                lineSentiment = - vs\n",
    "        print(\"This particular lines sentiment is: {}\".format(lineSentiment))\n",
    "        lines_sentiEN.append(lineSentiment)\n",
    "\n",
    "\n",
    "for act in tokensFR:\n",
    "    for j, line in enumerate(act):\n",
    "        lineSentiment = 0\n",
    "        print(\"line {} which contains: {}\".format(j, line))\n",
    "        for i in line:\n",
    "            vs = tb(i).sentiment[1]\n",
    "            if (vs > 0):\n",
    "                print(\"word \"+i+\" is positive\")\n",
    "                senti_listFR.append('Positive')\n",
    "                lineSentiment = + vs\n",
    "            elif (vs < 0):\n",
    "                print(\"word \"+i+\" is negative\")\n",
    "                senti_listFR.append('Negative')\n",
    "                lineSentiment = - vs\n",
    "            else:\n",
    "                #print(\"word \"+i+\" is neutral\")\n",
    "                senti_listFR.append('Neutral')\n",
    "                lineSentiment = - vs\n",
    "        print(\"This particular lines sentiment is: {}\".format(lineSentiment))\n",
    "        lines_sentiFR.append(lineSentiment)\n",
    "\n",
    "\n",
    "dataEN = senti_listEN\n",
    "dataSentiEN = lines_sentiEN\n",
    "\n",
    "dataFR = senti_listFR\n",
    "dataSentiFR = lines_sentiFR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=dataEN, name=\"EN\"))\n",
    "fig.add_trace(go.Histogram(x=dataFR, name=\"FR\"))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout()\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig3 = px.line(dataSentiEN, title='Line sentiment in English')\n",
    "fig4 = px.line(dataSentiFR, title='Line sentiment in French')\n",
    "\n",
    "\n",
    "fig3.show()\n",
    "fig4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.draw.dispersion import dispersion_plot\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "sortedCountEN = Counter(wordsEN)\n",
    "sortedCountFR = Counter(wordsFR)\n",
    "\n",
    "topN = 5\n",
    "\n",
    "\n",
    "# Todo: fuse to 1 plot? -was difficult to make details out\n",
    "plot = plt.figure(figsize=(12, 9))\n",
    "targetsEN = [word for word, cnt in sortedCountEN.most_common(topN)]\n",
    "dispersion_plot(wordsEN, targetsEN, ignore_case=True,\n",
    "                title='Lexical Dispersion Plot EN')\n",
    "\n",
    "plot2 = plt.figure(figsize=(12, 9))\n",
    "targetsFR = [word for word, cnt in sortedCountFR.most_common(topN)]\n",
    "dispersion_plot(wordsFR, targetsFR, ignore_case=True,\n",
    "                title='Lexical Dispersion Plot FR', )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 8\n",
    "\n",
    ">8.\tConsider the 5 most common tokens (excluding the stopwords, numbering and uncommon characters) for each corpus. Draw the histogram showing the number of occurrences of each these tokens. Use a two-bar representation to the scores for English and French corpus. The equivalence of the histograms would indicate the one-to-one equivalence of these common words too. Now we want to evaluate the ordering of appearance of these common words. Write a script that would allow you to determine the proportion of permutations that co-occur in English and French corpus. This is somehow similar to kendal’s tau, which quantifies correlation between some reference ordering and automatic order (Check Python statistical library for further information). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def remove_act_separation(wordlist):\n",
    "    final_list = []\n",
    "    for act in wordlist:\n",
    "        for line in act:\n",
    "            for word in line:\n",
    "                final_list.append(word)\n",
    "    return final_list\n",
    "            \n",
    "    \n",
    "#Counter() creates a dictionary with the words as keys and their frequency as values\n",
    "freqEN = Counter(remove_act_separation(noStopwEN))\n",
    "freqFR = Counter(remove_act_separation(noStopwFR))\n",
    "\n",
    "# top 100 most common words\n",
    "top100EN = freqEN.most_common(100)\n",
    "top100FR = freqFR.most_common(100)\n",
    "\n",
    "# top 5 words in each language\n",
    "#ill mother\n",
    "top5EN = freqEN.most_common(5)\n",
    "top5FR = freqFR.most_common(5)\n",
    "\n",
    "#make subplots\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].bar(range(len(top5EN)), [val[1] for val in top5EN], align='center')\n",
    "axs[0].set_title('5 Most common tokens in English corpus')\n",
    "axs[0].set_xticks(range(len(top5EN)))\n",
    "axs[0].set_xticklabels([val[0] for val in top5EN])  # give labels to x-axis\n",
    "axs[1].bar(range(len(top5FR)), [val[1] for val in top5FR], align='center')\n",
    "axs[1].set_title('5 Most common tokens in French corpus')\n",
    "axs[1].set_xticks(range(len(top5FR)))\n",
    "axs[1].set_xticklabels([val[0] for val in top5FR])\n",
    "plt.show()\n",
    "\n",
    "# top 5 words for english = sir, good, man, ill, mother\n",
    "# equivalent for french = monsieur, bon(ne), homme, mal, mere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 9\n",
    "\n",
    ">9.\tWe want to see how the common tokens vary in terms of number of syllables. You may use python library Pyphen, to determine the number of syllables of each word. Write a script that calculates the number of syllables for the 100 common tokens in both corpus, and suggest a diagram how the two corpuses compare in terms of number of syllables of their most common tokens.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "num_of_syllablesEN = []\n",
    "num_of_syllablesFR = []\n",
    "\n",
    "# count syllables in each word\n",
    "for token in top100EN:\n",
    "    num_of_syllablesEN.append(pyphen.Pyphen(lang='en').inserted(token[0]).count('-') + 1)\n",
    "for token in top100FR:\n",
    "    num_of_syllablesFR.append(pyphen.Pyphen(lang='fr').inserted(token[0]).count('-') + 1)\n",
    "\n",
    "# Count the frequency of syllables\n",
    "syllable_freqEN = Counter(num_of_syllablesEN)\n",
    "syllable_freqFR = Counter(num_of_syllablesFR)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].bar(range(len(syllable_freqEN)), [val for val in syllable_freqEN.values()], align='center')\n",
    "axs[0].set_title('Syllable frequency in English corpus')\n",
    "axs[0].set_xticks(range(len(syllable_freqEN)))\n",
    "axs[0].set_xticklabels([val for val in syllable_freqEN.keys()])\n",
    "axs[1].bar(range(len(syllable_freqFR)), [val for val in syllable_freqFR.values()], align='center')\n",
    "axs[1].set_title('Syllable frequency in French corpus')\n",
    "axs[1].set_xticks(range(len(syllable_freqFR)))\n",
    "axs[1].set_xticklabels([val for val in syllable_freqFR.keys()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9e2dcbae43fffae690d5e156edd28fcdf20860b79def7249b43b9217b9540be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
